# ZeroClaw Fork — Docker Compose Stack
#
# Two services:
#   1. zeroclaw: The fork binary (built from source, dev target)
#   2. ollama:   Local LLM server (Qwen 3 14B)
#
# Inbound webhook traffic (Linear/GitHub) is routed via Tailscale Funnel on the host.
# Tailscale routes public HTTPS traffic → host port 8080 → zeroclaw container.
#
# Quick start:
#   1. Install Tailscale and enable Funnel (see README or docs/planning/):
#        tailscale funnel --bg 8080
#      Copy the public URL (https://<host>.<tailnet>.ts.net) for the Linear webhook.
#   2. cp config/config.example.toml config/config.toml
#      # fill in bot_token, app_token, linear.api_key, webhook_signing_secret, etc.
#   3. docker compose -f docker-compose.fork.yml up -d
#   4. docker compose -f docker-compose.fork.yml exec ollama ollama pull qwen3:14b

services:

  # ── ZeroClaw Agent ──────────────────────────────────────────────────────────
  zeroclaw:
    build:
      context: .
      dockerfile: Dockerfile
      target: dev
    container_name: zeroclaw-fork
    restart: unless-stopped

    # Run in daemon mode: starts gateway + Slack Socket Mode + webhook server.
    command: ["daemon"]

    # Slack/Linear secrets live in config/config.toml (bind-mounted below).
    volumes:
      # Fork config (bind-mount): copy config/config.example.toml → config/config.toml
      # and fill in real secrets before starting.
      - ./config/config.toml:/zeroclaw-data/.zeroclaw/config.toml:ro
      # Named volume first so the bind mount below takes precedence for modes/.
      - zeroclaw-workspace:/zeroclaw-data/workspace
      # PM persona files: aieos_path and skills_dir resolve relative to workspace_dir.
      # Bind-mounted after the named volume so it shadows the workspace/modes/ path.
      - ./modes:/zeroclaw-data/workspace/modes:ro

    # Port 8080 is bound to localhost only so Tailscale Funnel (running on the host)
    # can route inbound webhook traffic to it. Not exposed to the wider network.
    # Port 42617 (gateway) stays internal — used only for the Docker healthcheck.
    expose:
      - "42617"  # ZeroClaw gateway (internal healthcheck target)
    ports:
      - "127.0.0.1:8080:8080"  # webhook listener — Tailscale Funnel → host:8080 → container

    depends_on:
      ollama:
        condition: service_healthy
    networks:
      - fork-net

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:42617/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s

    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 256M

  # ── Ollama LLM Server ───────────────────────────────────────────────────────
  ollama:
    image: ollama/ollama:latest
    container_name: zeroclaw-ollama
    restart: unless-stopped
    volumes:
      - ollama-models:/root/.ollama
    # Internal-only: zeroclaw reaches Ollama via http://zeroclaw-ollama:11434
    expose:
      - "11434"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 10s
      timeout: 5s
      retries: 6
      start_period: 20s
    deploy:
      resources:
        reservations:
          memory: 10G
    networks:
      - fork-net

networks:
  fork-net:
    driver: bridge

volumes:
  zeroclaw-workspace:
  ollama-models:
