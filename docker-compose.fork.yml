# ZeroClaw Fork — Docker Compose Stack
#
# Three services:
#   1. zeroclaw: The fork binary (built from source, dev target)
#   2. ollama:   Local LLM server (Qwen 3 14B)
#   3. tunnel:   Cloudflare Tunnel for Linear/GitHub webhooks
#
# Quick start:
#   1. cp config/config.example.toml config/config.toml
#      # fill in SLACK_BOT_TOKEN, SLACK_APP_TOKEN, LINEAR_API_KEY, etc.
#   2. cp .env.fork.example .env.fork
#      # fill in CLOUDFLARE_TUNNEL_TOKEN
#   3. docker compose -f docker-compose.fork.yml up -d
#   4. docker compose -f docker-compose.fork.yml exec ollama ollama pull qwen3:14b
#
# First-time Ollama setup:
#   docker compose -f docker-compose.fork.yml exec ollama ollama pull qwen3:14b

services:

  # ── ZeroClaw Agent ──────────────────────────────────────────────────────────
  zeroclaw:
    build:
      context: .
      dockerfile: Dockerfile
      target: dev
    container_name: zeroclaw-fork
    restart: unless-stopped

    # Run in daemon mode: starts gateway + Slack Socket Mode + webhook server.
    command: ["daemon"]

    # Slack/Linear secrets live in config/config.toml (bind-mounted below).
    # Compose-level variables (CLOUDFLARE_TUNNEL_TOKEN) live in .env.fork.
    env_file:
      - .env.fork

    volumes:
      # Fork config (bind-mount): copy config/config.example.toml → config/config.toml
      # and fill in real secrets before starting.
      - ./config/config.toml:/zeroclaw-data/.zeroclaw/config.toml:ro
      # PM persona files: aieos_path and skills_dir resolve relative to workspace_dir.
      - ./modes:/zeroclaw-data/workspace/modes:ro
      # Persistent workspace (memory, tool outputs, etc.)
      - zeroclaw-workspace:/zeroclaw-data/workspace

    # Webhook port is internal-only; cloudflared routes external traffic to it.
    # The gateway port is also internal-only (no public exposure needed for the fork).
    expose:
      - "42617"  # ZeroClaw gateway (internal health check target)
      - "8080"   # Linear/GitHub webhook listener

    depends_on:
      ollama:
        condition: service_started
    networks:
      - fork-net

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:42617/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s

    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 256M

  # ── Ollama LLM Server ───────────────────────────────────────────────────────
  ollama:
    image: ollama/ollama:latest
    container_name: zeroclaw-ollama
    restart: unless-stopped
    volumes:
      - ollama-models:/root/.ollama
    # Internal-only: zeroclaw reaches Ollama via http://zeroclaw-ollama:11434
    expose:
      - "11434"
    deploy:
      resources:
        reservations:
          memory: 10G
    networks:
      - fork-net

  # ── Cloudflare Tunnel ───────────────────────────────────────────────────────
  #
  # Routes inbound Linear/GitHub webhook traffic to zeroclaw-fork:8080.
  # Tunnel token and public hostname routing are configured in the
  # Cloudflare Zero Trust dashboard (Networks → Tunnels → your tunnel).
  #
  # Dashboard route to add:
  #   Public Hostname:  webhooks.<your-domain>
  #   Service:          http://zeroclaw-fork:8080
  tunnel:
    image: cloudflare/cloudflared:latest
    container_name: zeroclaw-tunnel
    restart: unless-stopped
    command: tunnel --no-autoupdate run
    environment:
      - TUNNEL_TOKEN=${CLOUDFLARE_TUNNEL_TOKEN}
    depends_on:
      zeroclaw:
        condition: service_healthy
    networks:
      - fork-net

networks:
  fork-net:
    driver: bridge

volumes:
  zeroclaw-workspace:
  ollama-models:
